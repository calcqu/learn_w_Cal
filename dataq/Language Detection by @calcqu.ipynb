{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP LEARNING WITH KERAS, AN EXAMPLE\n",
    "\n",
    "This is an example of using deep learning (a subset of machine learning), to model rules and patterns to transform passed in inputs to desired outputs.  \n",
    "\n",
    "This post is written above an introductory level, while aiming to remain accessible, interesting and hands-on.\n",
    "\n",
    "Let’s take a small data set and attempt to reach reasonable predictions…\n",
    "\n",
    "The idea is to take a text snippet and identify a class by tagging a target label.\n",
    "\n",
    "The type of text snippets are 3 to 7 word phrases, examples like: end of the road, go for the green, hard at work.\n",
    "The objective is to predict the language of the text, hence this is a classification problem.\n",
    "\n",
    "The 15 language categories are:\n",
    "French, Italian, Bangla, Tagalog, Spanish, Japanese, Korean, Hindi, German, Greek, Somali, Portuguese, Czech, Croatian and Romanian\n",
    "\n",
    "The dataset: 20 phrases for training, which is what the model will learn from (`trainx.txt` & `trainy.txt`), 2 phrases for validation, which is the first chance to see how well the model generalizes to unseen data (`valx.txt` & `valy.txt`) and 1 phrase to test the accuracy of the classifier (`testx.txt` & `testy.txt`).\n",
    "\n",
    "Using the Keras library in Python enables quick prototyping and has many built in features for experimenting with neural network architectures.  The architecture is the network design describing how to go from input to output.  Neural nets can be constructed in a variety of ways similar to the floor plan in an office or store, or the layout for orchestrating traffic (think jug-handles, turn lanes, overpass ramps, or round-about systems).  Back on topic…\n",
    "\n",
    "\n",
    "**Let’s access the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"text\":\"maître de la maison\"}\\n', '{\"text\":\"padrone di casa\"}\\n']\n",
      "['{\"classification\":\"fr\"}\\n', '{\"classification\":\"it\"}\\n']\n"
     ]
    }
   ],
   "source": [
    "trainx = open('trainx.txt', 'r', encoding = \"utf8\")\n",
    "trainx = trainx.readlines()\n",
    "\n",
    "valx = open('valx.txt', 'r', encoding = \"utf8\")\n",
    "valx = valx.readlines()\n",
    "\n",
    "testx = open('testx.txt', 'r', encoding = \"utf8\")\n",
    "testx = testx.readlines()\n",
    "\n",
    "trainy = open('trainy.txt', 'r', encoding = \"utf8\")\n",
    "trainy = trainy.readlines()\n",
    "\n",
    "valy = open('valy.txt', 'r', encoding = \"utf8\")\n",
    "valy = valy.readlines()\n",
    "\n",
    "testy = open('testy.txt', 'r', encoding = \"utf8\")\n",
    "testy = testy.readlines()\n",
    "\n",
    "print(trainx[0:2])\n",
    "print(trainy[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Opening the files, and looking at the first few records, some clean up is required to remove all text that is not part of the input phrase or target language label. This is handled by defining a function `multipleReplace` and specifying the characters to discard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maître de la maison\n",
      "fr\n"
     ]
    }
   ],
   "source": [
    "def multipleReplace(text, wordDict):\n",
    "    for key in wordDict:\n",
    "        text = text.replace(key, wordDict[key])\n",
    "    return text\n",
    "\n",
    "rep_x = {'{\"text\":\"': '', '\"}': '', '\\n': ''} \n",
    "\n",
    "train_x=[]\n",
    "for i in range(len(trainx)):\n",
    "    w = multipleReplace(trainx[i], rep_x)\n",
    "    train_x.append(w)\n",
    "    \n",
    "val_x=[]\n",
    "for i in range(len(valx)):\n",
    "    w = multipleReplace(valx[i], rep_x)\n",
    "    val_x.append(w)\n",
    "    \n",
    "test_x=[]\n",
    "for i in range(len(testx)):\n",
    "    w = multipleReplace(testx[i], rep_x)\n",
    "    test_x.append(w)\n",
    "\n",
    "rep_y = {'{\"classification\":\"': '', '\"}\\n': '', '\"}': ''}\n",
    "\n",
    "train_y=[]\n",
    "for i in range(len(trainy)):\n",
    "    w = multipleReplace(trainy[i], rep_y)\n",
    "    train_y.append(w)\n",
    "    \n",
    "val_y=[]\n",
    "for i in range(len(valy)):\n",
    "    w = multipleReplace(valy[i], rep_y)\n",
    "    val_y.append(w)\n",
    "\n",
    "test_y=[]\n",
    "for i in range(len(testy)):\n",
    "    w = multipleReplace(testy[i], rep_y)\n",
    "    test_y.append(w)\n",
    "\n",
    "print(train_x[0])\n",
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has a tokenizer which will be explored further below in the post, but first lets use nltk (you may need to run `nltk.download(‘punkt’)` before `import nltk`).  Tokenizing the phrase, means parsing the statement for the individual words often referred to as tokens.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['maître', 'de', 'la', 'maison'], 'fr'), (['padrone', 'di', 'casa'], 'it')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "train_wrds = []\n",
    "train_doc = []\n",
    "\n",
    "for i in range(len(train_x)):\n",
    "    wd = nltk.word_tokenize(train_x[i])\n",
    "    train_wrds.extend(wd)\n",
    "    train_doc.append((wd, train_y[i]))\n",
    "\n",
    "train_wrds = sorted(list(set(train_wrds)))\n",
    "train_doc[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step merges the tokens of a phrase with the language label in a list called `train_doc`, and also creates a unique list of the individual tokens.\n",
    "\n",
    "The next code block identifies the number of languages as class categories, and builds a dictionary to one-hot-encode the language labels with a binary indicator representation as to where among the language categories a particular language is.  One Hot Encoding is a way of numerically representing ‘yes’ and ‘no’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'be': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'cr': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'cz': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " 'fr': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       " 'ge': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'gr': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       " 'in': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'it': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'ja': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " 'ko': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'pg': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'ro': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'so': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " 'sp': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       " 'ta': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = set(train_y)\n",
    "\n",
    "one_hot_classes = []\n",
    "empty_output = [0] * len(classes)\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    en = list(empty_output)\n",
    "    en[i] = 1\n",
    "    one_hot_classes.append(en)\n",
    "\n",
    "dds = {}  #create dictionary - alternate method discussed further below\n",
    "dds = zip(classes, one_hot_classes)\n",
    "dds = dict(dds)\n",
    "tag = list(dds.keys())\n",
    "dds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step is to change the `train_doc` from words to numbers.  \n",
    "\n",
    "The phrase snippet will now be represented as a bag of words, where every word from every phrase creates a universe of words we will call a bag.  For each phrase, or record instance row in data talk, a 1 for yes will appear where that word is positioned in the bag, and 0’s in all other positions.  \n",
    "\n",
    "As this bag is large, a simple illustration is useful.  \n",
    "\n",
    "`[‘I like green’, ‘I like blue’]`\n",
    "\n",
    "If these are the only two phrases, there are 4 unique words in the bag, `'I'`, `'like'`, `'green'`, and `'blue'`.\n",
    "\n",
    "For a new statement `'I like red'`, the column positions for `'I'` and `'like'` will be switched on to yes as 1’s where the other two word positions in the bag, `'blue'` and `'green'`, will display 0.  \n",
    "\n",
    "`[‘I like red’] >> [1, 1, 0, 0]`\n",
    "\n",
    "Similarly the target label is transformed from a text description of say 'be' for Bangla, to the [1, 0, 0, … 0] representation as in the dictionary previously created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training= []\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "for dc in train_doc:\n",
    "    bag = []  \n",
    "    token_words = dc[0] \n",
    "    for ws in train_wrds:\n",
    "        bag.append(1) if ws in token_words else bag.append(0)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[tag.index(dc[1])] = 1\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "training[0][1]  #the [1] index signals the output below will display the target label\n",
    "#training[0][0] will display only the phrase as a one hot encoded representation (one column for each word in the bag)\n",
    "#training[0] will display both the phrase and target for the first record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have transformed the text in `train_doc` to a representation in numbers and call this list `training`.\n",
    "\n",
    "The first record `(['maître', 'de', 'la', 'maison'], 'fr')` is now `([0, 0, ...,0], [0, 0, ...,0])`\n",
    "\n",
    "The next code block shuffles the records, and splits them into two, the first is the lists of phrases as `xtrain` and second list is the corresponding labels as `ytrain`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(training)\n",
    "\n",
    "xtrain = []\n",
    "ytrain = []\n",
    "\n",
    "for i in range(len(training)):\n",
    "    xx = training[i][0]\n",
    "    yy = training[i][1]\n",
    "    xtrain.append(xx)\n",
    "    ytrain.append(yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly the same steps are repeated for the validation data phrases and the test data phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['la', 'notte', 'dei', 'morti', 'viventi'], 'it'),\n",
       " (['noaptea', 'mortilor', 'vii'], 'ro')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_wrds = []\n",
    "val_doc = []\n",
    "\n",
    "for i in range(len(val_x)):\n",
    "    wd = nltk.word_tokenize(val_x[i])\n",
    "    val_wrds.extend(wd)\n",
    "    val_doc.append((wd, val_y[i]))\n",
    "    \n",
    "val_wrds = sorted(list(set(val_wrds)))\n",
    "\n",
    "test_wrds = []\n",
    "test_doc = []\n",
    "\n",
    "for i in range(len(test_x)):\n",
    "    wd = nltk.word_tokenize(test_x[i])\n",
    "    test_wrds.extend(wd)\n",
    "    test_doc.append((wd, test_y[i]))\n",
    "    \n",
    "test_wrds = sorted(list(set(test_wrds)))\n",
    "\n",
    "val_doc[0:2]\n",
    "test_doc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val = [] \n",
    "test = [] \n",
    "\n",
    "for dc in val_doc:\n",
    "    bag = []  \n",
    "    token_words = dc[0] \n",
    "    for ws in train_wrds:\n",
    "        bag.append(1) if ws in token_words else bag.append(0)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[tag.index(dc[1])] = 1\n",
    "    val.append([bag, output_row])\n",
    "    \n",
    "for dc in test_doc:\n",
    "    bag = []  \n",
    "    token_words = dc[0] \n",
    "    for ws in train_wrds:\n",
    "        bag.append(1) if ws in token_words else bag.append(0)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[tag.index(dc[1])] = 1\n",
    "    test.append([bag, output_row])\n",
    "    \n",
    "random.shuffle(val)\n",
    "\n",
    "xval = []\n",
    "yval = []\n",
    "\n",
    "for i in range(len(val)):\n",
    "    xx = val[i][0]\n",
    "    yy = val[i][1]\n",
    "    xval.append(xx)\n",
    "    yval.append(yy)\n",
    "\n",
    "xtest = []\n",
    "ytest = []\n",
    "\n",
    "for i in range(len(test)):\n",
    "    xx = test[i][0]\n",
    "    yy = test[i][1]\n",
    "    xtest.append(xx)\n",
    "    ytest.append(yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has all been elaborate steps of pre-processing data work, also known as preparing the data for modeling and in the format best accepted by the neural network.  \n",
    "\n",
    "Let’s now construct the neural network architecture.\n",
    "\n",
    "**Attention Please**\n",
    "\n",
    "Keras provides a `Sequential` API, which operates like pushing dominoes, where one triggers the next in a sequential order.  There is also a Functional API in Keras which allows for multiple inputs, multiple output, and otherwise architectures that are not restricted to flow sequentially.  Do not let the term API scare you, as it merely implies a functionality that we import in to use.  \n",
    "\n",
    "`Dense` and `Dropout` are used, where dense simply means the use of a fully connected layer and dropout is a form of regularization.  Fully connected implies each node is connected to each node in the previous and following layers. That’s it.  Dropout is a way to deselect some nodes from influencing the learning and subsequently the decision output of the model.  Think of dropout as when the smartest kids in class are absent, and the other students can not depend on the absentees to quickly raise their hands and supply answers (lol).\n",
    "\n",
    "The architecture can be altered with more or less layers, more or less neurons, a different optimizer, a different loss function, a different learning rate, varying learning rate decay, different activation functions and kernel initializers.  This is all considered hyper parameter tuning and model construction.  This post aims at demonstrating an example instead of going thru each component of machine learning. \n",
    "\n",
    "Neurons are the number of nodes or connections in a layer.  In the first layer of the architecture this is 10000, and the second layer has 5000 neurons.  With respect to choosing the number of neurons, lets interpret each layer as focusing on a specific aspect of learning, while each neuron is focusing on aspects of that specific aspect (apologies for getting all fortune cookie on you).  Onward…\n",
    "\n",
    "The loss is simply how the neural network evaluates how well it is performing, and the optimizer seeks to minimize the loss, and this is checked by looking at the accuracy.  The compiler is the conclusion of the network architecture.  \n",
    "\n",
    "Next, the model is `fit` to the data, which means trained with the input phrases and the associated language labels.  In this example, a separate data set has been reserved for validation, but the validation can be done by partitioning some of the training data instead.\n",
    "\n",
    "`scores` looks at the test data, and evaluates how the model performs to unseen data.  Note, the `evaluate` method requires the inputs to be numpy arrays and not lists.  \n",
    "\n",
    "`predict_classes` enables the `classification_report` to display how well each category was predicted.  \n",
    "\n",
    "Let’s look at the actual predictions in the next code block and compare them with the answers.  The phrase in the test data set is “word of the day”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras backend :  tensorflow\n",
      "Train on 300 samples, validate on 30 samples\n",
      "Epoch 1/3\n",
      " - 4s - loss: 2.8009 - acc: 0.1200 - val_loss: 2.4462 - val_acc: 0.2667\n",
      "Epoch 2/3\n",
      " - 1s - loss: 1.2266 - acc: 0.7200 - val_loss: 1.8725 - val_acc: 0.4000\n",
      "Epoch 3/3\n",
      " - 1s - loss: 0.2990 - acc: 0.9733 - val_loss: 1.5209 - val_acc: 0.5667\n",
      "15/15 [==============================] - 0s 894us/step\n",
      "On Test Data acc: 53.33%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         be       0.00      0.00      0.00         1\n",
      "         pg       1.00      1.00      1.00         1\n",
      "         it       0.00      0.00      0.00         1\n",
      "         ro       0.00      0.00      0.00         1\n",
      "         ko       0.20      1.00      0.33         1\n",
      "         in       0.00      0.00      0.00         1\n",
      "         cr       0.00      0.00      0.00         1\n",
      "         ge       0.50      1.00      0.67         1\n",
      "         ta       1.00      1.00      1.00         1\n",
      "         gr       1.00      1.00      1.00         1\n",
      "         fr       0.00      0.00      0.00         1\n",
      "         so       0.50      1.00      0.67         1\n",
      "         cz       0.00      0.00      0.00         1\n",
      "         ja       1.00      1.00      1.00         1\n",
      "         sp       1.00      1.00      1.00         1\n",
      "\n",
      "avg / total       0.41      0.53      0.44        15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\calc8\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#This is the architecture below\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "print(\"Keras backend : \", keras.backend.backend())\n",
    "\n",
    "input_size = len(train_wrds)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10000,input_dim=input_size,kernel_initializer=\"uniform\",activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5000,kernel_initializer=\"uniform\",activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(classes),kernel_initializer=\"uniform\",activation=\"softmax\"))\n",
    "model_optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.001 / 3)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=model_optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#This is the architecture above\n",
    "\n",
    "history = model.fit(xtrain, ytrain,\n",
    "          epochs=3,\n",
    "          validation_data=(xval, yval),\n",
    "          batch_size=32,\n",
    "          verbose=2,\n",
    "          shuffle=True)\n",
    "#history outputs the model training\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "scores = model.evaluate(np.array(xtest), np.array(ytest), verbose=1)\n",
    "print(\"On Test Data %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "y_pred = model.predict_classes(xtest)\n",
    "y_prd = keras.utils.to_categorical(y_pred, num_classes = len(classes))\n",
    "target_names = list(dds.keys())\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(np.array(ytest), y_prd, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have just ran a neural network!**\n",
    "\n",
    "Performance is okay for initial pass, and in future posts we can refine this approach and explore more advanced methods.  \n",
    "\n",
    "Three learning passes at the data (epochs), allowed the model to understand the training data well, but the validation data was not able to mirror such performance.  With each pass the network improves, via reduced loss, and rising accuracy for both the training and validation sets.  The test set is on par with the validation accuracy, which is reasonable given the small dataset.  \n",
    "\n",
    "Lets qualify performance.  On a previous assignment, the model I created reached 96% and I was asked if that was good.  It was terrible!  96% means 1 out of 32 approximately is incorrect.  99% would imply 1 out of 100 is incorrect, and 99.5% would imply 1 out of 200 is incorrect.  The standard for performance is high.  This data set is extremely small with just 20 phrases to train on, so generally more data will help improve the learning.\n",
    "\n",
    "*Note, once a model has been trained, it can be saved and re-loaded later for use.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n",
      "ko\n",
      "so\n",
      "gr\n",
      "ko\n",
      "sp\n",
      "ta\n",
      "ko\n",
      "ge\n",
      "pg\n",
      "ko\n",
      "so\n",
      "ja\n",
      "ko\n",
      "ge\n"
     ]
    }
   ],
   "source": [
    "#To see the literal predictions\n",
    "\n",
    "y_p = model.predict(xtest)\n",
    "y_p = y_p.argmax(axis=1)\n",
    "real_Pred=[]\n",
    "def label():\n",
    "    for i in range(len(y_p)):\n",
    "        p = y_p[i]\n",
    "        pred = tag[int(p)]\n",
    "        print(pred)\n",
    "        real_Pred.append(pred)\n",
    "\n",
    "lb = label() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'ro',\n",
       " 'so',\n",
       " 'gr',\n",
       " 'ko',\n",
       " 'sp',\n",
       " 'ta',\n",
       " 'be',\n",
       " 'fr',\n",
       " 'pg',\n",
       " 'cr',\n",
       " 'in',\n",
       " 'ja',\n",
       " 'cz',\n",
       " 'ge']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To see the actual test language labels to compare to above predictions\n",
    "test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the two code block outputs above, shows the first two were wrong, followed by the next five being correct, and ultimately 8 of 15 being correct.\n",
    "\n",
    "You may have noticed the bag will only get bigger with more samples, and the sparse matrix filled predominantly with zeros will take more space and memory, and quickly become inefficient.  Instead of ‘one hot encoding’ the input data, let’s try ‘integer encoding’ the input data so that a finite number of columns can be used, saving space and memory.  \n",
    "\n",
    "Let’s use Keras’ `Tokenizer`, `pad_sequences` and `to_catgeorical` (for one hot encoding), and refer back to the uploaded data, stripped of everything but the phrases and labels, `train_x` and `train_y`.\n",
    "\n",
    "The max phrase size is 7 words and let’s limit the integer encoding to 1000, which we call `top_words`.\n",
    "\n",
    "`Tokenizer` has some useful functionality, such as methods: `fit_on_texts` which creates a vocabulary, `texts_to_sequences` which integer encodes, and `word_index` which creates a dictionary. **Wow!\n",
    "\n",
    "`pad_sequences` normalizes the numeric representation for situations of phrases varying between 3 and 7 words, so that the result is each integer encoded sequence will not be the same length of columns. \n",
    "\n",
    "Using the same methods, with the exception of `fit_to_texts` again, allows the validation and test data to reference the same bag of words for indexing, thus creating the desired same relationships.\n",
    "\n",
    "That takes care of the inputs.  For the outputs, scikit library provides `label_encoder` to transform the language label to an integer, and using Keras’ `to_categorical` alters to one-hot-encoding format (scikit has `label_binarizer` which does this as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 668 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from keras.utils import to_categorical\n",
    "\n",
    "max_phrase_size = 7 #maximum length of the sentence\n",
    "embedding_vecor_length = 3\n",
    "top_words = 1000\n",
    "\n",
    "#for inputs - integer encoding\n",
    "tokenizer = Tokenizer(top_words)\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "seqns = tokenizer.texts_to_sequences(train_x)\n",
    "word_index = tokenizer.word_index\n",
    "xtrain2 = pad_sequences(seqns, max_phrase_size)\n",
    "\n",
    "seqns2 = tokenizer.texts_to_sequences(val_x)\n",
    "xval2 = pad_sequences(seqns2, max_phrase_size)\n",
    "\n",
    "seqns3 = tokenizer.texts_to_sequences(test_x)\n",
    "xtest2 = pad_sequences(seqns3, max_phrase_size)\n",
    "\n",
    "#for label outputs\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_y)\n",
    "encoded_Y = encoder.transform(train_y)\n",
    "ytrain2 = to_categorical(encoded_Y)\n",
    "\n",
    "encoded_Y2 = encoder.transform(val_y)\n",
    "yval2 = to_categorical(encoded_Y2)\n",
    "\n",
    "encoded_Y3 = encoder.transform(test_y)\n",
    "ytest2 = to_categorical(encoded_Y3)\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at a training example in phrase format, padded sequence format, and one-hot-encoded label format to see we have the desired data structure for neural networks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maître de la maison\n",
      "dueño de la tienda\n",
      "[ 0  0  0 98  1  4 46]\n",
      "[  0   0   0  48   1   4 146]\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0])\n",
    "print(train_x[31])\n",
    "\n",
    "print(xtrain2[0])  # can easily set to be a variable and divide by /len(word_index) to max min scale\n",
    "print(xtrain2[31]) # can easily set to be a variable and divide by /len(word_index) to max min scale\n",
    "\n",
    "print(ytrain2[0])\n",
    "print(ytrain2[31])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the efficient on space, integer encoding creates relationships of distance between numbers that should not exist, hence the practice of one-hot-encoding.  \n",
    "\n",
    "This data format as is, DOES NOT perform as well as the first attempt…in the current architecture format that is. (Even after max-min scaling, which reduced the integers to values between 0 and 1)\n",
    "\n",
    "An embedding layer and matrix is a more advanced technique to explore in another post.\n",
    "\n",
    "To test your knowledge so far I encourage you to re-run the neural network architecture with these integer encoded input data. \n",
    "\n",
    "\n",
    "**But we are not done yet.  One more approach, to provide maximum value to the readers.**\n",
    "\n",
    "I pulled 50 commonly used words associated with the subject of school, then web scraped the language transformations to create a training data set.  Voila! This is `swlx.txt` & `swly.txt`\n",
    "\n",
    "The objective is to create a vocabulary of these 50 school related words in each of the 15 languages, and train a model to take in any school related word, phrase, sentence or more and predict the language.  The idea is, it is expected the network will perform well when using a sentence that includes one of the 50 words trained on, regardless of the other words in the sentence.  Also the model should learn some relationships for when a word is the same in multiple languages, but perhaps lose some accuracy.  And it will be interesting to see if the model learns to make decent predictions on non-school related inputs. \n",
    "\n",
    "Let’s get to it, before the editors cut me off (lol).\n",
    "\n",
    "Run the same cleaning operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examen\n",
      "fr\n"
     ]
    }
   ],
   "source": [
    "swx = open('swlx.txt', 'r', encoding = \"utf8\")\n",
    "swx = swx.readlines()\n",
    "\n",
    "rep_x = {'{\"text\":\"': '', '\"}': '', '\\n': ''} \n",
    "\n",
    "sw_x=[]\n",
    "for i in range(len(swx)):\n",
    "    w = multipleReplace(swx[i], rep_x)\n",
    "    sw_x.append(w)\n",
    "\n",
    "print(sw_x[0])\n",
    "\n",
    "swy = open('swly.txt', 'r', encoding = \"utf8\")\n",
    "swy = swy.readlines()\n",
    "\n",
    "rep_y = {'{\"classification\":\"': '', '\"}\\n': '', '\"}': ''}\n",
    "\n",
    "sw_y=[]\n",
    "for i in range(len(swy)):\n",
    "    w = multipleReplace(swy[i], rep_y)\n",
    "    sw_y.append(w)\n",
    "\n",
    "print(sw_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer` has a method `fit_transform` which will absorb the vocabulary of our school related tokens.  The method `toarray` one-hot-encodes, which is being used for the input data. \n",
    "\n",
    "The `label_encoder` method, used above, translates the language labels to numbers and we one-hot-encode the output data. You are getting the hang of it now.  To satisfy any curiosity we display integer encoded labels and see the range is 0-14 corresponding to the 15 languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  7,  0, 14, 13,  8,  9,  6,  4,  5, 12, 10,  2,  1, 11,  3,  7,\n",
       "        0, 14, 13,  8,  9,  6,  4,  5, 12, 10,  2,  1, 11,  3,  7,  0, 14,\n",
       "       13,  8,  9,  6,  4,  5, 12, 10,  2,  1, 11,  3,  7,  0, 14, 13,  8,\n",
       "        9,  6,  4,  5, 12, 10,  2,  1, 11,  3,  7,  0, 14, 13,  8,  9,  6,\n",
       "        4,  5, 12, 10,  2,  1, 11,  3,  7,  0, 14, 13,  8,  9,  6,  4,  5,\n",
       "       12, 10,  2,  1, 11,  3,  7,  0, 14, 13,  8,  9,  6,  4,  5, 12, 10,\n",
       "        2,  1, 11,  3,  7,  0, 14, 13,  8,  9,  6,  4,  5, 12, 10,  2,  1,\n",
       "       11,  3,  7,  0, 14, 13,  8,  9,  6,  4,  5, 12, 10,  2,  1, 11,  3,\n",
       "        7,  0, 14, 13,  8,  9,  6,  4,  5, 12, 10,  2,  1, 11,  3,  7,  0,\n",
       "       14, 13,  8,  9,  6,  4,  5, 12, 10,  2,  1, 11,  3,  7,  0, 14, 13,\n",
       "        8,  9,  6,  4,  5, 12, 10,  2,  1, 11,  3,  7,  0, 14, 13,  8,  9,\n",
       "        6,  4,  5, 12, 10,  2,  1, 11,  3,  7,  0, 14, 13,  8,  9,  6,  4,\n",
       "        5, 12, 10,  2,  1, 11,  3,  7,  0, 14, 13,  8,  9,  6,  4,  5, 12,\n",
       "       10,  2,  1, 11,  3,  7,  0, 14, 13,  8,  9,  6,  4,  5, 12, 10,  2,\n",
       "        1, 11,  3,  7,  0, 14, 13,  8,  9,  6,  4,  5, 12, 10,  2,  1, 11,\n",
       "        3,  7,  0, 14, 13,  8,  9,  6,  4,  5, 12, 10,  2,  1, 11,  3,  7,\n",
       "        0, 14, 13,  8,  9,  6,  4,  5, 12, 10,  2,  1, 11,  3,  7,  0, 14,\n",
       "       13,  8,  9,  6,  4,  5, 12, 10,  2,  1, 11,  3,  7,  0, 14, 13,  8,\n",
       "        9,  6,  4,  5, 12, 10,  2,  1, 11,  3,  7,  0, 14, 13,  8,  9,  6,\n",
       "        4,  5, 12, 10,  2,  1, 11,  3,  7,  0, 14, 13,  8,  9,  6,  4,  5,\n",
       "       12, 10,  2,  1, 11,  3,  7,  0, 14, 13,  8,  9,  6,  4,  5, 12, 10,\n",
       "        2,  1, 11,  3,  7,  0, 14, 13,  8,  9,  6,  4, 12, 10,  2,  1, 11,\n",
       "        5,  3,  7,  0, 14, 13,  8,  9,  6,  4,  5, 12, 10,  2,  1, 11,  3,\n",
       "        7,  0, 14, 13,  8,  9,  6,  4,  5, 12, 10,  2,  1, 11,  3,  7,  0,\n",
       "       14, 13,  8,  9,  6,  4,  5, 12, 10,  2,  1, 11,  3,  7,  0, 14, 13,\n",
       "        8,  9,  6,  4,  5, 12, 10,  2,  1, 11,  3,  7,  0, 14, 13,  9,  4,\n",
       "        5, 12, 10,  2,  1, 11,  8,  6,  3,  7,  0, 14, 13,  8,  9,  6,  4,\n",
       "        5, 12, 10,  2,  1, 11,  3,  7,  0, 14, 13,  8,  9,  6,  4,  5, 12,\n",
       "       10,  2,  1, 11,  3,  7,  0, 14, 13,  8,  9,  6,  4,  5, 12, 10,  2,\n",
       "        1, 11,  3,  7,  0, 14, 13,  8,  9,  6,  4,  5, 12, 10,  2,  1, 11,\n",
       "        3,  7,  0, 14, 13,  8,  9,  6,  4,  5, 12, 10,  2,  1, 11,  3,  7,\n",
       "        0, 14, 13,  8,  9,  6,  4,  5, 12, 10,  2,  1, 11,  3,  7,  0, 14,\n",
       "       13,  8,  9,  6,  4,  5, 12, 10,  2,  1, 11,  3,  7,  0, 14, 13,  8,\n",
       "        9,  6,  4,  5, 12, 10,  2,  1, 11,  3,  7,  0, 14, 13,  8,  9,  6,\n",
       "        4,  5, 12, 10,  2,  1, 11,  3,  7,  0, 14, 13,  8,  9,  6,  4,  5,\n",
       "       12, 10,  2,  1, 11,  3,  7,  0, 14, 13,  8,  9,  6,  4,  5, 12, 10,\n",
       "        2,  1, 11,  3,  7,  0, 14, 13,  8,  9,  6,  4,  5, 12, 10,  2,  1,\n",
       "       11,  3,  7,  0, 14, 13,  8,  9,  6,  4,  5, 12, 10,  2,  1, 11,  3,\n",
       "        7,  0, 14, 13,  8,  9,  6,  4,  5, 12, 10,  2,  1, 11,  3,  7,  0,\n",
       "       14, 13,  8,  9,  6,  4,  5, 12, 10,  2,  1, 11,  3,  7,  0, 14, 13,\n",
       "        8,  9,  6,  4,  5, 12, 10,  2,  1, 11,  3,  7,  0, 14, 13,  8,  9,\n",
       "        6,  4,  5, 12, 10,  2,  1, 11,  3,  7,  0, 14, 13,  8,  9,  6,  4,\n",
       "        5, 12, 10,  2,  1, 11,  3,  7,  0, 14, 13,  8,  9,  6,  4,  5, 12,\n",
       "       10,  2,  1, 11,  3,  7,  0, 14, 13,  8,  9,  6,  4,  5, 12, 10,  2,\n",
       "        1, 11,  3,  7,  0, 14, 13,  8,  9,  6,  4,  5, 12, 10,  2,  1, 11], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "tags = vect.fit_transform(sw_x)\n",
    "\n",
    "xtrains = tags.toarray()\n",
    "xtrains.shape\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "encode = LabelEncoder()\n",
    "encode.fit(sw_y)\n",
    "encode_y = encode.transform(sw_y)\n",
    "ytrains = to_categorical(encode_y)\n",
    "ytrains.shape\n",
    "encode_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the same neural net architecture as above… (no validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " - 2s - loss: 2.8778 - acc: 0.0523\n",
      "Epoch 2/5\n",
      " - 1s - loss: 2.0551 - acc: 0.4915\n",
      "Epoch 3/5\n",
      " - 1s - loss: 0.9212 - acc: 0.8902\n",
      "Epoch 4/5\n",
      " - 1s - loss: 0.3421 - acc: 0.9124\n",
      "Epoch 5/5\n",
      " - 1s - loss: 0.3367 - acc: 0.9020\n"
     ]
    }
   ],
   "source": [
    "modeln = Sequential()\n",
    "modeln.add(Dense(10000,input_dim=xtrains.shape[1],kernel_initializer=\"uniform\",activation=\"relu\"))\n",
    "modeln.add(Dropout(0.5))\n",
    "modeln.add(Dense(5000,kernel_initializer=\"uniform\",activation=\"relu\"))\n",
    "modeln.add(Dropout(0.5))\n",
    "modeln.add(Dense(15,kernel_initializer=\"uniform\",activation=\"softmax\"))\n",
    "model_optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.001 / 5)\n",
    "modeln.compile(loss='categorical_crossentropy',\n",
    "              optimizer=model_optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "historyn = modeln.fit(xtrains, ytrains,\n",
    "          epochs=5,\n",
    "          batch_size=32,\n",
    "          verbose=2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets go straight to prediction, but this time, enter a word, phrase or sentence in one of the languages and lets evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fr'],\n",
       "      dtype='<U2')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trythis = ['quelle école allez-vous fréquenter']\n",
    "\n",
    "#quelle école allez-vous fréquenter\n",
    "#se poio scholeío tha parevretheíte\n",
    "#eotteon haggyoe danil yejeong-ingayo\n",
    "\n",
    "#Āmi ē'i sēmisṭārē bharti karaba ēbaṁ sam'māna saṅgē snātaka habē\n",
    "#main is semestar mein naamaankan aur sammaan ke saath snaatak hone kee yojana bana raha hoon\n",
    "#vou inscrever este semestre e planejo graduar com honras\n",
    "#Ich werde dieses Semester einschreiben und planen, mit Ehren zu absolvieren\n",
    "\n",
    "#permite studierea programelor și cumpărarea manualelor\n",
    "#studia il programma e compra i libri di testo\n",
    "#hinahayaan ang pag-aaral ng syllabus at bumili ng mga aklat-aralin\n",
    "\n",
    "new_ = vect.transform(trythis)\n",
    "\n",
    "use_ =new_.toarray()\n",
    "\n",
    "y_n = modeln.predict(use_)\n",
    "\n",
    "y_n = y_n.argmax(axis=1)\n",
    "y_n\n",
    "\n",
    "predictions_for_trythis = encode.inverse_transform(y_n)\n",
    "predictions_for_trythis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about “test your knowledge” (lol) by creating a test data set and setting up predict_classes.  \n",
    "\n",
    "Some sample sentences have been commented out that you can copy-paste into trythis =[‘…’] to play with the tool. \n",
    "\n",
    "Wink Emoji\n",
    "\n",
    "Spend your time getting smarter, follow me on Twitter @calcqu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
